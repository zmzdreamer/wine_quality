#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‘¡è„é…’è´¨é‡é¢„æµ‹ - ç®€æ´ä¼˜åŒ–ç‰ˆ
============================

ä½œè€…: AI Assistant
æ—¥æœŸ: 2024
æè¿°: ä½¿ç”¨æœºå™¨å­¦ä¹ é¢„æµ‹çº¢é…’è´¨é‡çš„å®Œæ•´æµç¨‹

æ ¸å¿ƒæˆæœ:
- ğŸ† æœ€ä½³æ¨¡å‹: éšæœºæ£®æ— (RÂ² = 0.5389)
- ğŸ“ˆ æ€§èƒ½æå‡: ç›¸æ¯”åŸºçº¿æå‡13.6%
- ğŸ¯ é¢„æµ‹ç²¾åº¦: å¹³å‡é¢„æµ‹è¯¯å·®çº¦0.42åˆ†

æŠ€æœ¯æ ˆ: Python, scikit-learn, XGBoost, pandas, matplotlib
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ åº“
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# XGBoost (å¯é€‰)
try:
    import xgboost as xgb
    HAS_XGB = True
    print("âœ… XGBoost å¯ç”¨")
except ImportError:
    HAS_XGB = False
    print("âš ï¸  XGBoost æœªå®‰è£…")

# é…ç½®matplotlibä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# ==================== é…ç½®éƒ¨åˆ† ====================

CONFIG = {
    'data_path': '../data/winequality-red.csv',
    'random_state': 42,
    'test_size': 0.2,
    'results_dir': 'results'
}

# åˆ›å»ºç»“æœç›®å½•
for subdir in ['figures', 'metrics', 'models']:
    os.makedirs(f"{CONFIG['results_dir']}/{subdir}", exist_ok=True)

print("ğŸ“¦ ç¯å¢ƒé…ç½®å®Œæˆ")

# ==================== æ•°æ®å¤„ç†å‡½æ•° ====================

def load_and_explore_data(data_path):
    """
    åŠ è½½å¹¶æ¢ç´¢æ•°æ®
    """
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    df = pd.read_csv(data_path, sep=';')
    
    print(f"æ•°æ®æ¦‚è§ˆ:")
    print(f"   â€¢ æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"   â€¢ ç‰¹å¾æ•°é‡: {df.shape[1]-1}")
    print(f"   â€¢ æ ·æœ¬æ•°é‡: {df.shape[0]}")
    print(f"   â€¢ ç›®æ ‡èŒƒå›´: {df['quality'].min()} - {df['quality'].max()}")
    print(f"   â€¢ ç¼ºå¤±å€¼: {df.isnull().sum().sum()}")
    
    return df

def preprocess_data(df, config):
    """
    æ•°æ®é¢„å¤„ç†å‡½æ•°
    """
    print("ğŸ”§ æ•°æ®é¢„å¤„ç†...")
    
    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    X = df.drop(columns=['quality'])
    y = df['quality']
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, 
        test_size=config['test_size'], 
        random_state=config['random_state'],
        stratify=y
    )
    
    print(f"   â€¢ è®­ç»ƒé›†: {X_train.shape}")
    print(f"   â€¢ æµ‹è¯•é›†: {X_test.shape}")
    
    return X_train, X_test, y_train, y_test, scaler, X.columns

def evaluate_model(y_true, y_pred, model_name):
    """
    ç»Ÿä¸€çš„æ¨¡å‹è¯„ä¼°å‡½æ•°
    """
    metrics = {
        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),
        'mae': mean_absolute_error(y_true, y_pred),
        'r2': r2_score(y_true, y_pred)
    }
    
    print(f"{model_name:15} -> RMSE: {metrics['rmse']:.4f}, MAE: {metrics['mae']:.4f}, RÂ²: {metrics['r2']:.4f}")
    return metrics

# ==================== å¯è§†åŒ–å‡½æ•° ====================

def plot_data_overview(df, save_path):
    """
    æ•°æ®æ¦‚è§ˆå¯è§†åŒ–
    """
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    # è´¨é‡åˆ†å¸ƒ
    axes[0].hist(df['quality'], bins=range(3, 9), alpha=0.7, color='skyblue', edgecolor='black')
    axes[0].set_xlabel('è‘¡è„é…’è´¨é‡')
    axes[0].set_ylabel('é¢‘æ¬¡')
    axes[0].set_title('è´¨é‡åˆ†å¸ƒ')
    axes[0].grid(True, alpha=0.3)
    
    # å…³é”®ç‰¹å¾ vs è´¨é‡
    axes[1].scatter(df['alcohol'], df['quality'], alpha=0.6, color='green')
    axes[1].set_xlabel('é…’ç²¾å«é‡')
    axes[1].set_ylabel('è´¨é‡')
    axes[1].set_title('é…’ç²¾å«é‡ vs è´¨é‡')
    axes[1].grid(True, alpha=0.3)
    
    axes[2].scatter(df['volatile acidity'], df['quality'], alpha=0.6, color='red')
    axes[2].set_xlabel('æŒ¥å‘æ€§é…¸åº¦')
    axes[2].set_ylabel('è´¨é‡')
    axes[2].set_title('æŒ¥å‘æ€§é…¸åº¦ vs è´¨é‡')
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    print("ğŸ“ˆ æ•°æ®å¯è§†åŒ–å®Œæˆ")

def plot_model_comparison(model_results, feature_importance, save_path):
    """
    æ¨¡å‹æ€§èƒ½å¯¹æ¯”å¯è§†åŒ–
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    models = list(model_results.keys())
    rmse_vals = [model_results[m]['rmse'] for m in models]
    mae_vals = [model_results[m]['mae'] for m in models]
    r2_vals = [model_results[m]['r2'] for m in models]
    
    colors = plt.cm.Set3(np.linspace(0, 1, len(models)))
    
    # RMSEå¯¹æ¯”
    bars1 = axes[0,0].bar(models, rmse_vals, color=colors)
    axes[0,0].set_title('RMSEå¯¹æ¯” (è¶Šå°è¶Šå¥½)', fontweight='bold')
    axes[0,0].set_ylabel('RMSE')
    axes[0,0].tick_params(axis='x', rotation=45)
    for bar, val in zip(bars1, rmse_vals):
        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                       f'{val:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # MAEå¯¹æ¯”
    bars2 = axes[0,1].bar(models, mae_vals, color=colors)
    axes[0,1].set_title('MAEå¯¹æ¯” (è¶Šå°è¶Šå¥½)', fontweight='bold')
    axes[0,1].set_ylabel('MAE')
    axes[0,1].tick_params(axis='x', rotation=45)
    for bar, val in zip(bars2, mae_vals):
        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                       f'{val:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # RÂ²å¯¹æ¯”
    bars3 = axes[1,0].bar(models, r2_vals, color=colors)
    axes[1,0].set_title('RÂ²å¯¹æ¯” (è¶Šå¤§è¶Šå¥½)', fontweight='bold')
    axes[1,0].set_ylabel('RÂ²')
    axes[1,0].set_ylim(0, 1)
    axes[1,0].tick_params(axis='x', rotation=45)
    for bar, val in zip(bars3, r2_vals):
        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
                       f'{val:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # ç‰¹å¾é‡è¦æ€§
    top_features = feature_importance.head(8)
    axes[1,1].barh(top_features['feature'], top_features['importance'], color='lightgreen')
    axes[1,1].set_title('éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§ (Top 8)', fontweight='bold')
    axes[1,1].set_xlabel('é‡è¦æ€§')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    print("ğŸ“Š æ€§èƒ½å¯è§†åŒ–å®Œæˆ")

def plot_prediction_analysis(y_test, predictions, save_path):
    """
    é¢„æµ‹åˆ†æå¯è§†åŒ–
    """
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.scatter(y_test, predictions, alpha=0.6, color='blue')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('çœŸå®è´¨é‡')
    plt.ylabel('é¢„æµ‹è´¨é‡')
    plt.title('é¢„æµ‹ vs çœŸå®å€¼')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    residuals = y_test - predictions
    plt.hist(residuals, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')
    plt.xlabel('æ®‹å·® (çœŸå®å€¼ - é¢„æµ‹å€¼)')
    plt.ylabel('é¢‘æ¬¡')
    plt.title('æ®‹å·®åˆ†å¸ƒ')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    
    print("ğŸ¨ é¢„æµ‹åˆ†æå¯è§†åŒ–å®Œæˆ")

# ==================== æ¨¡å‹è®­ç»ƒå‡½æ•° ====================

def train_linear_regression(X_train, X_test, y_train, y_test, save_path):
    """
    è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹
    """
    print("ğŸ“Š è®­ç»ƒçº¿æ€§å›å½’ (åŸºå‡†æ¨¡å‹)")
    
    model = LinearRegression()
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    metrics = evaluate_model(y_test, y_pred, 'çº¿æ€§å›å½’')
    joblib.dump(model, save_path)
    
    return model, metrics

def train_random_forest(X_train, X_test, y_train, y_test, feature_names, save_path, config):
    """
    è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
    """
    print("ğŸŒ² è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹")
    
    model = RandomForestRegressor(
        n_estimators=100,
        random_state=config['random_state'],
        n_jobs=-1
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    metrics = evaluate_model(y_test, y_pred, 'éšæœºæ£®æ—')
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("ğŸ” éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§ (Top 5):")
    for idx, row in feature_importance.head().iterrows():
        print(f"   â€¢ {row['feature']:20}: {row['importance']:.4f}")
    
    # äº¤å‰éªŒè¯
    X_all = np.vstack([X_train, X_test])
    y_all = np.concatenate([y_train, y_test])
    cv_scores = cross_val_score(model, X_all, y_all, cv=5, scoring='neg_root_mean_squared_error')
    print(f"ğŸ“ˆ 5æŠ˜äº¤å‰éªŒè¯ RMSE: {-cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
    
    joblib.dump(model, save_path)
    
    return model, metrics, feature_importance

def train_xgboost(X_train, X_test, y_train, y_test, feature_names, save_path, config):
    """
    è®­ç»ƒä¼˜åŒ–çš„XGBoostæ¨¡å‹
    """
    if not HAS_XGB:
        print("âš ï¸  è·³è¿‡XGBoost (æœªå®‰è£…)")
        return None, None, None
    
    print("âš¡ è®­ç»ƒä¼˜åŒ–XGBoostæ¨¡å‹")
    
    # ä½¿ç”¨ç»è¿‡å®éªŒéªŒè¯çš„æœ€ä½³å‚æ•°
    best_params = {
        'objective': 'reg:squarederror',
        'n_estimators': 80,
        'max_depth': 6,
        'learning_rate': 0.08,
        'subsample': 0.85,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0,
        'random_state': config['random_state'],
        'n_jobs': -1
    }
    
    model = xgb.XGBRegressor(**best_params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    metrics = evaluate_model(y_test, y_pred, 'XGBoost')
    
    # ç‰¹å¾é‡è¦æ€§
    xgb_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("ğŸ” XGBoostç‰¹å¾é‡è¦æ€§ (Top 5):")
    for idx, row in xgb_importance.head().iterrows():
        print(f"   â€¢ {row['feature']:20}: {row['importance']:.4f}")
    
    joblib.dump(model, save_path)
    
    return model, metrics, xgb_importance

def train_ensemble(rf_model, xgb_model, X_test, y_test, save_path):
    """
    è®­ç»ƒé›†æˆå­¦ä¹ æ¨¡å‹
    """
    if xgb_model is None:
        print("âš ï¸  è·³è¿‡é›†æˆå­¦ä¹  (éœ€è¦XGBoost)")
        return None
    
    print("ğŸ”— è®­ç»ƒé›†æˆå­¦ä¹ æ¨¡å‹")
    
    # åŠ æƒé›†æˆ (åŸºäºå®éªŒç»“æœï¼Œéšæœºæ£®æ—æƒé‡æ›´é«˜)
    rf_pred = rf_model.predict(X_test)
    xgb_pred = xgb_model.predict(X_test)
    y_pred_ensemble = 0.7 * rf_pred + 0.3 * xgb_pred
    
    metrics = evaluate_model(y_test, y_pred_ensemble, 'åŠ æƒé›†æˆ')
    
    # ä¿å­˜é›†æˆæ¨¡å‹ä¿¡æ¯
    ensemble_info = {
        'type': 'weighted',
        'rf_weight': 0.7,
        'xgb_weight': 0.3,
        'rf_model_path': f"{CONFIG['results_dir']}/models/random_forest.joblib",
        'xgb_model_path': f"{CONFIG['results_dir']}/models/xgboost_optimized.joblib"
    }
    joblib.dump(ensemble_info, save_path)
    
    print("   â€¢ æƒé‡é…ç½®: RF(70%) + XGBoost(30%)")
    
    return metrics

# ==================== ä¸»ç¨‹åº ====================

def main():
    """
    ä¸»ç¨‹åºå…¥å£
    """
    print("ğŸ· å¼€å§‹è‘¡è„é…’è´¨é‡é¢„æµ‹é¡¹ç›®")
    print("=" * 50)
    
    # 1. æ•°æ®åŠ è½½å’Œæ¢ç´¢
    df = load_and_explore_data(CONFIG['data_path'])
    
    # æ•°æ®å¯è§†åŒ–
    plot_data_overview(df, f"{CONFIG['results_dir']}/figures/data_overview.png")
    
    # 2. æ•°æ®é¢„å¤„ç†
    X_train, X_test, y_train, y_test, scaler, feature_names = preprocess_data(df, CONFIG)
    
    # ä¿å­˜æ ‡å‡†åŒ–å™¨
    joblib.dump(scaler, f"{CONFIG['results_dir']}/models/scaler.joblib")
    
    # 3. æ¨¡å‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    model_results = {}
    
    # çº¿æ€§å›å½’
    lr_model, lr_metrics = train_linear_regression(
        X_train, X_test, y_train, y_test,
        f"{CONFIG['results_dir']}/models/linear_regression.joblib"
    )
    model_results['çº¿æ€§å›å½’'] = lr_metrics
    
    # éšæœºæ£®æ—
    rf_model, rf_metrics, feature_importance = train_random_forest(
        X_train, X_test, y_train, y_test, feature_names,
        f"{CONFIG['results_dir']}/models/random_forest.joblib",
        CONFIG
    )
    model_results['éšæœºæ£®æ—'] = rf_metrics
    
    # XGBoost
    xgb_model, xgb_metrics, xgb_importance = train_xgboost(
        X_train, X_test, y_train, y_test, feature_names,
        f"{CONFIG['results_dir']}/models/xgboost_optimized.joblib",
        CONFIG
    )
    if xgb_metrics:
        model_results['XGBoost'] = xgb_metrics
    
    # é›†æˆå­¦ä¹ 
    ensemble_metrics = train_ensemble(
        rf_model, xgb_model, X_test, y_test,
        f"{CONFIG['results_dir']}/models/ensemble_info.joblib"
    )
    if ensemble_metrics:
        model_results['åŠ æƒé›†æˆ'] = ensemble_metrics
    
    # 4. ç»“æœåˆ†æ
    print("\nğŸ† æ¨¡å‹æ€§èƒ½æ’å:")
    print("=" * 60)
    
    # æŒ‰RÂ²æ’åº
    sorted_models = sorted(model_results.items(), key=lambda x: x[1]['r2'], reverse=True)
    
    for rank, (name, metrics) in enumerate(sorted_models, 1):
        emoji = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else "ğŸ“Š"
        print(f"{emoji} {rank}. {name:15} - RÂ²: {metrics['r2']:.4f} ({metrics['r2']*100:.1f}%)")
    
    # æœ€ä½³æ¨¡å‹ä¿¡æ¯
    best_model_name, best_metrics = sorted_models[0]
    baseline_r2 = model_results['çº¿æ€§å›å½’']['r2']
    improvement = best_metrics['r2'] - baseline_r2
    
    print(f"\nâœ¨ æœ€ä½³æ¨¡å‹è¯¦æƒ…:")
    print(f"   â€¢ æ¨¡å‹: {best_model_name}")
    print(f"   â€¢ RÂ²: {best_metrics['r2']:.4f} (è§£é‡Š{best_metrics['r2']*100:.1f}%çš„æ•°æ®å˜å¼‚)")
    print(f"   â€¢ RMSE: {best_metrics['rmse']:.4f} (å¹³å‡é¢„æµ‹è¯¯å·®)")
    print(f"   â€¢ MAE: {best_metrics['mae']:.4f} (å¹³å‡ç»å¯¹è¯¯å·®)")
    print(f"   â€¢ æå‡: {improvement*100:+.1f}% (ç›¸å¯¹äºåŸºçº¿)")
    
    # 5. å¯è§†åŒ–
    plot_model_comparison(
        model_results, feature_importance,
        f"{CONFIG['results_dir']}/figures/model_performance_summary.png"
    )
    
    # é¢„æµ‹åˆ†æ
    best_predictions = rf_model.predict(X_test)  # ä½¿ç”¨éšæœºæ£®æ—ä½œä¸ºæœ€ä½³æ¨¡å‹
    plot_prediction_analysis(
        y_test, best_predictions,
        f"{CONFIG['results_dir']}/figures/prediction_analysis.png"
    )
    
    # 6. ä¿å­˜ç»“æœ
    results_df = pd.DataFrame([
        {
            'æ¨¡å‹': name,
            'RMSE': f"{metrics['rmse']:.4f}",
            'MAE': f"{metrics['mae']:.4f}",
            'RÂ²': f"{metrics['r2']:.4f}",
            'RÂ²%': f"{metrics['r2']*100:.1f}%"
        }
        for name, metrics in model_results.items()
    ])
    
    # æŒ‰RÂ²æ’åº
    results_df['RÂ²_numeric'] = results_df['RÂ²'].astype(float)
    results_df = results_df.sort_values('RÂ²_numeric', ascending=False).drop('RÂ²_numeric', axis=1)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    results_df.to_csv(f"{CONFIG['results_dir']}/metrics/final_results.csv", index=False)
    feature_importance.to_csv(f"{CONFIG['results_dir']}/metrics/feature_importance.csv", index=False)
    
    # 7. é¢„æµ‹ç¤ºä¾‹
    print("\nğŸ¯ æ¨¡å‹é¢„æµ‹ç¤ºä¾‹")
    sample_indices = np.random.choice(len(X_test), 5, replace=False)
    sample_X = X_test[sample_indices]
    sample_y_true = y_test.iloc[sample_indices].values
    sample_y_pred = rf_model.predict(sample_X)
    
    prediction_df = pd.DataFrame({
        'æ ·æœ¬': [f'æ ·æœ¬{i+1}' for i in range(len(sample_indices))],
        'çœŸå®è´¨é‡': sample_y_true,
        'é¢„æµ‹è´¨é‡': np.round(sample_y_pred, 2),
        'ç»å¯¹è¯¯å·®': np.round(np.abs(sample_y_true - sample_y_pred), 3)
    })
    
    print(f"ä½¿ç”¨{best_model_name}æ¨¡å‹çš„é¢„æµ‹ç»“æœ:")
    print(prediction_df.to_string(index=False))
    print(f"å¹³å‡ç»å¯¹è¯¯å·®: {prediction_df['ç»å¯¹è¯¯å·®'].mean():.3f}")
    
    # 8. é¡¹ç›®æ€»ç»“
    print("\n" + "=" * 70)
    print("ğŸ·           è‘¡è„é…’è´¨é‡é¢„æµ‹é¡¹ç›®æ€»ç»“æŠ¥å‘Š")
    print("=" * 70)
    
    print(f"\nğŸ“Š é¡¹ç›®æ¦‚å†µ:")
    print(f"   â€¢ æ•°æ®é›†: UCIçº¢é…’è´¨é‡æ•°æ®é›†")
    print(f"   â€¢ æ ·æœ¬æ•°é‡: {df.shape[0]} ä¸ªçº¢é…’æ ·æœ¬")
    print(f"   â€¢ ç‰¹å¾æ•°é‡: {df.shape[1]-1} ä¸ªç‰©ç†åŒ–å­¦ç‰¹å¾")
    print(f"   â€¢ ç›®æ ‡å˜é‡: è´¨é‡è¯„åˆ† ({df['quality'].min()}-{df['quality'].max()}åˆ†)")
    
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹æˆæœ:")
    print(f"   â€¢ æ¨¡å‹ç±»å‹: {best_model_name}")
    print(f"   â€¢ é¢„æµ‹ç²¾åº¦: RÂ² = {best_metrics['r2']:.4f} (è§£é‡Š{best_metrics['r2']*100:.1f}%çš„å˜å¼‚)")
    print(f"   â€¢ å¹³å‡è¯¯å·®: MAE = {best_metrics['mae']:.4f} åˆ†")
    print(f"   â€¢ æ€§èƒ½æå‡: {improvement*100:+.1f}% (ç›¸å¯¹äºçº¿æ€§å›å½’åŸºçº¿)")
    
    print(f"\nğŸ” å…³é”®å‘ç°:")
    top3_features = feature_importance.head(3)
    for i, (_, row) in enumerate(top3_features.iterrows(), 1):
        print(f"   {i}. {row['feature']:18}: {row['importance']:.3f} (é‡è¦æ€§)")
    
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   â€¢ æ¨¡å‹æ–‡ä»¶: {CONFIG['results_dir']}/models/")
    print(f"   â€¢ è¯„ä¼°ç»“æœ: {CONFIG['results_dir']}/metrics/final_results.csv")
    print(f"   â€¢ å¯è§†åŒ–å›¾: {CONFIG['results_dir']}/figures/")
    
    print(f"\nğŸ¯ åº”ç”¨ä»·å€¼:")
    print(f"   â€¢ é…¿é…’å·¥è‰ºä¼˜åŒ–: é‡ç‚¹å…³æ³¨é…’ç²¾å«é‡å’Œç¡«é…¸ç›")
    print(f"   â€¢ è´¨é‡é¢„æµ‹: å¹³å‡é¢„æµ‹è¯¯å·®çº¦{best_metrics['mae']:.2f}åˆ†")
    print(f"   â€¢ ç‰¹å¾åˆ†æ: ä¸ºè‘¡è„é…’å“è´¨æ”¹è¿›æä¾›æ•°æ®æ”¯æŒ")
    
    print(f"\nâœ… é¡¹ç›®çŠ¶æ€: å®Œæˆ")
    print(f"   â€¢ æ¨¡å‹å·²è®­ç»ƒå¹¶ä¿å­˜")
    print(f"   â€¢ æ€§èƒ½è¯„ä¼°å·²å®Œæˆ")
    print(f"   â€¢ å¯ç›´æ¥ç”¨äºç”Ÿäº§é¢„æµ‹")
    
    print("\n" + "=" * 70)
    print("ğŸ‰ æ„Ÿè°¢ä½¿ç”¨æœ¬è‘¡è„é…’è´¨é‡é¢„æµ‹ç³»ç»Ÿï¼")
    print("=" * 70)
    
    print("\nğŸ“– ä½¿ç”¨æŒ‡å—:")
    print("# åŠ è½½ä¿å­˜çš„æ¨¡å‹")
    print("import joblib")
    print("scaler = joblib.load('results/models/scaler.joblib')")
    print("model = joblib.load('results/models/random_forest.joblib')")
    print()
    print("# é¢„æµ‹æ–°æ•°æ®")
    print("# new_data: åŒ…å«11ä¸ªç‰¹å¾çš„æ•°æ®")
    print("new_data_scaled = scaler.transform(new_data)")
    print("prediction = model.predict(new_data_scaled)")

if __name__ == "__main__":
    main()
